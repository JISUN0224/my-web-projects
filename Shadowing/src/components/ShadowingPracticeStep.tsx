import React, { useState, useEffect, useRef } from 'react';
import ScriptDisplay from './ScriptDisplay';
import AudioControls from './AudioControls';
import { evaluatePronunciationWithAzure } from '../utils/azureSpeechUtils';

interface ShadowingPracticeStepProps {
  text: string;
  onGoBack: () => void;
  onEvaluate: (audioBlob: Blob) => void;
}

interface AudioState {
  isPlaying: boolean;
  isRecording: boolean;
  hasRecording: boolean;
  currentWordIndex: number;
  audioBlob: Blob | null;
}

const ShadowingPracticeStep: React.FC<ShadowingPracticeStepProps> = ({
  text,
  onGoBack,
  onEvaluate
}) => {
  const [audioState, setAudioState] = useState<AudioState>({
    isPlaying: false,
    isRecording: false,
    hasRecording: false,
    currentWordIndex: -1,
    audioBlob: null
  });

  const [pinyin, setPinyin] = useState<string>('');
  const [isLoadingAudio, setIsLoadingAudio] = useState(false);
  
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const synthesizedAudioRef = useRef<HTMLAudioElement | null>(null);
  const highlightTimeoutRef = useRef<NodeJS.Timeout | null>(null);

  // ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸ ì‹œ ë³‘ìŒ ìƒì„± ë° TTS ìŒì„± ìƒì„±
  useEffect(() => {
    generatePinyin();
    generateTTSAudio();
  }, [text]);

  // ë³‘ìŒ ìƒì„± (ì‹¤ì œë¡œëŠ” API í˜¸ì¶œ)
  const generatePinyin = () => {
    // ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Azure Translatorë‚˜ ë‹¤ë¥¸ ì¤‘êµ­ì–´ ì²˜ë¦¬ API ì‚¬ìš©
    const mockPinyin = convertToPinyin(text);
    setPinyin(mockPinyin);
  };

  // TTS ìŒì„± ìƒì„±
  const generateTTSAudio = async () => {
    setIsLoadingAudio(true);
    try {
      // Azure Speech Services API í˜¸ì¶œ
      const audioUrl = await generateAzureTTS(text);
      synthesizedAudioRef.current = new Audio(audioUrl);
      
      // ì˜¤ë””ì˜¤ ë¡œë“œ ì™„ë£Œ í›„ ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ì¶”ê°€
      synthesizedAudioRef.current.addEventListener('loadeddata', () => {
        console.log('Azure TTS ì˜¤ë””ì˜¤ ë¡œë“œ ì™„ë£Œ');
      });
      
      // ì˜¤ë””ì˜¤ ì¬ìƒ ì™„ë£Œ ì‹œ í•˜ì´ë¼ì´íŠ¸ ë¦¬ì…‹
      synthesizedAudioRef.current.addEventListener('ended', () => {
        setAudioState(prev => ({ ...prev, isPlaying: false, currentWordIndex: -1 }));
      });

      // ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬
      synthesizedAudioRef.current.addEventListener('error', (e) => {
        console.error('Azure TTS ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨:', e);
        alert('Azure ìŒì„± ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
      });
    } catch (error) {
      console.error('Azure TTS ìƒì„± ì‹¤íŒ¨:', error);
      alert('Azure ìŒì„± ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
    } finally {
      setIsLoadingAudio(false);
    }
  };

  // ìŒì„± ì¬ìƒ
  const playAudio = async () => {
    if (!synthesizedAudioRef.current) {
      alert('ìŒì„±ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
      return;
    }

    setAudioState(prev => ({ ...prev, isPlaying: true }));
    
    try {
      // ì˜¤ë””ì˜¤ê°€ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
      if (synthesizedAudioRef.current.readyState < 2) {
        await new Promise((resolve, reject) => {
          const timeout = setTimeout(() => {
            reject(new Error('ì˜¤ë””ì˜¤ ë¡œë“œ ì‹œê°„ ì´ˆê³¼'));
          }, 5000);
          
          synthesizedAudioRef.current!.addEventListener('canplaythrough', () => {
            clearTimeout(timeout);
            resolve(true);
          }, { once: true });
          
          synthesizedAudioRef.current!.addEventListener('error', () => {
            clearTimeout(timeout);
            reject(new Error('ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨'));
          }, { once: true });
        });
      }
      
      await synthesizedAudioRef.current.play();
      startTextHighlight();
    } catch (error) {
      console.error('ì˜¤ë””ì˜¤ ì¬ìƒ ì‹¤íŒ¨:', error);
      setAudioState(prev => ({ ...prev, isPlaying: false }));
      
      if (error instanceof Error) {
        if (error.message.includes('ì‚¬ìš©ì')) {
          alert('ë¸Œë¼ìš°ì €ì—ì„œ ì˜¤ë””ì˜¤ ì¬ìƒì„ ì°¨ë‹¨í–ˆìŠµë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.');
        } else {
          alert('ìŒì„± ì¬ìƒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
        }
      } else {
        alert('ìŒì„± ì¬ìƒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
      }
    }
  };

  // ìŒì„± ì¤‘ì§€
  const stopAudio = () => {
    if (synthesizedAudioRef.current) {
      synthesizedAudioRef.current.pause();
      synthesizedAudioRef.current.currentTime = 0;
    }
    
    if (highlightTimeoutRef.current) {
      clearTimeout(highlightTimeoutRef.current);
    }
    
    setAudioState(prev => ({ 
      ...prev, 
      isPlaying: false, 
      currentWordIndex: -1 
    }));
  };

  // í…ìŠ¤íŠ¸ í•˜ì´ë¼ì´íŠ¸ ì• ë‹ˆë©”ì´ì…˜
  const startTextHighlight = () => {
    const words = text.replace(/[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š]/g, ' ').split(/\s+/).filter(word => word.length > 0);
    let currentIndex = 0;

    const highlightNext = () => {
      if (currentIndex < words.length && audioState.isPlaying) {
        setAudioState(prev => ({ ...prev, currentWordIndex: currentIndex }));
        currentIndex++;
        
        // ê° ë‹¨ì–´ë‹¹ ì•½ 500ms ê°„ê²©ìœ¼ë¡œ í•˜ì´ë¼ì´íŠ¸ (ì‹¤ì œë¡œëŠ” ìŒì„± ê¸¸ì´ì— ë§ì¶° ì¡°ì •)
        highlightTimeoutRef.current = setTimeout(highlightNext, 500);
      }
    };

    highlightNext();
  };

  // ë…¹ìŒ ì‹œì‘
  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        } 
      });
      
      // ì§€ì›ë˜ëŠ” ì˜¤ë””ì˜¤ í˜•ì‹ í™•ì¸
      const supportedTypes = [
        'audio/wav',
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/mp4'
      ];
      
      let selectedType = 'audio/webm;codecs=opus'; // ê¸°ë³¸ê°’
      
      for (const type of supportedTypes) {
        if (MediaRecorder.isTypeSupported(type)) {
          selectedType = type;
          break;
        }
      }
      
      console.log('ì„ íƒëœ ì˜¤ë””ì˜¤ í˜•ì‹:', selectedType);
      
      const options = {
        mimeType: selectedType,
        audioBitsPerSecond: 16000
      };
      
      mediaRecorderRef.current = new MediaRecorder(stream, options);
      audioChunksRef.current = [];

      mediaRecorderRef.current.ondataavailable = (event) => {
        audioChunksRef.current.push(event.data);
      };

      mediaRecorderRef.current.onstop = () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: selectedType });
        setAudioState(prev => ({ 
          ...prev, 
          hasRecording: true, 
          audioBlob 
        }));
        
        // ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
        stream.getTracks().forEach(track => track.stop());
      };

      mediaRecorderRef.current.start();
      setAudioState(prev => ({ ...prev, isRecording: true }));
    } catch (error) {
      console.error('ë…¹ìŒ ì‹œì‘ ì‹¤íŒ¨:', error);
      alert('ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.');
    }
  };

  // ë…¹ìŒ ì¤‘ì§€
  const stopRecording = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop();
      setAudioState(prev => ({ ...prev, isRecording: false }));
    }
  };

  // ë…¹ìŒ í† ê¸€
  const toggleRecording = () => {
    if (audioState.isRecording) {
      stopRecording();
    } else {
      startRecording();
    }
  };

  // ë°œìŒ í‰ê°€ ì‹œì‘
  const handleEvaluate = () => {
    if (audioState.audioBlob) {
      onEvaluate(audioState.audioBlob);
    }
  };

  return (
    <div className="space-y-8">
      <div className="text-center">
        <h2 className="text-3xl font-bold text-gray-800 mb-2">ì‰ë„ì‰ ì—°ìŠµ</h2>
        <p className="text-gray-600">ìŒì„±ì„ ë“£ê³  ë”°ë¼ ë§í•´ë³´ì„¸ìš”</p>
      </div>

      {/* ìŠ¤í¬ë¦½íŠ¸ í‘œì‹œ ì˜ì—­ */}
      <ScriptDisplay 
        text={text}
        currentWordIndex={audioState.currentWordIndex}
        isPlaying={audioState.isPlaying}
        audioElement={synthesizedAudioRef.current}
        onWordHighlight={(wordIndex) => {
          setAudioState(prev => ({ ...prev, currentWordIndex: wordIndex }));
        }}
      />

      {/* ì˜¤ë””ì˜¤ ì»¨íŠ¸ë¡¤ */}
      <AudioControls
        isPlaying={audioState.isPlaying}
        isRecording={audioState.isRecording}
        hasRecording={audioState.hasRecording}
        isLoadingAudio={isLoadingAudio}
        onPlay={playAudio}
        onStop={stopAudio}
        onToggleRecording={toggleRecording}
        onEvaluate={handleEvaluate}
      />

      {/* í•˜ë‹¨ ë„¤ë¹„ê²Œì´ì…˜ */}
      <div className="flex justify-between items-center pt-6">
        <button
          onClick={onGoBack}
          className="flex items-center space-x-2 px-6 py-3 bg-gray-500 text-white rounded-full hover:bg-gray-600 transition-all duration-300"
        >
          <span>â†</span>
          <span>ëŒì•„ê°€ê¸°</span>
        </button>

        <div className="text-center">
          {audioState.isRecording && (
            <div className="flex items-center space-x-2 text-red-500">
              <div className="w-3 h-3 bg-red-500 rounded-full animate-pulse"></div>
              <span className="text-sm font-medium">ë…¹ìŒ ì¤‘...</span>
            </div>
          )}
          {audioState.hasRecording && !audioState.isRecording && (
            <div className="flex items-center space-x-2 text-green-500">
              <span>âœ“</span>
              <span className="text-sm font-medium">ë…¹ìŒ ì™„ë£Œ</span>
            </div>
          )}
        </div>
      </div>
    </div>
  );
};

// ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë³„ë„ íŒŒì¼ë¡œ ë¶„ë¦¬)
const convertToPinyin = (text: string): string => {
  // ì‹¤ì œë¡œëŠ” ì¤‘êµ­ì–´ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë‚˜ API ì‚¬ìš©
  const pinyinMap: { [key: string]: string } = {
    'ä½ ': 'nÇ', 'å¥½': 'hÇo', 'æˆ‘': 'wÇ’', 'æ˜¯': 'shÃ¬', 'æ¥': 'lÃ¡i', 'è‡ª': 'zÃ¬',
    'éŸ©': 'hÃ¡n', 'å›½': 'guÃ³', 'çš„': 'de', 'å­¦': 'xuÃ©', 'ç”Ÿ': 'shÄ“ng',
    'æ­£': 'zhÃ¨ng', 'åœ¨': 'zÃ i', 'ä¹ ': 'xÃ­', 'ä¸­': 'zhÅng', 'æ–‡': 'wÃ©n'
  };
  
  return text.split('').map(char => pinyinMap[char] || char).join(' ');
};

const generateAzureTTS = async (text: string): Promise<string> => {
  // í™˜ê²½ ë³€ìˆ˜ ê²€ì¦
  const subscriptionKey = import.meta.env.VITE_AZURE_SPEECH_KEY;
  const region = import.meta.env.VITE_AZURE_SPEECH_REGION || 'eastasia';

  console.log('ğŸ” í™˜ê²½ë³€ìˆ˜ í™•ì¸:', {
    subscriptionKey: subscriptionKey ? `âœ… ì„¤ì •ë¨ (${subscriptionKey.substring(0, 10)}...)` : 'âŒ ì„¤ì •ë˜ì§€ ì•ŠìŒ',
    region: region ? `âœ… ì„¤ì •ë¨ (${region})` : 'âŒ ì„¤ì •ë˜ì§€ ì•ŠìŒ'
  });

  if (!subscriptionKey) {
    throw new Error('VITE_AZURE_SPEECH_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');
  }

  // XML ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
  const escapedText = text
    .replace(/&/g, '&amp;')
    .replace(/</g, '&lt;')
    .replace(/>/g, '&gt;')
    .replace(/"/g, '&quot;')
    .replace(/'/g, '&apos;');

  const ssml = `<speak version='1.0' xml:lang='zh-CN'><voice xml:lang='zh-CN' xml:gender='Male' name='zh-CN-YunyangNeural'>${escapedText}</voice></speak>`;

  try {
    // API í‚¤ ê²€ì¦ ë° ì •ë¦¬
    const cleanKey = subscriptionKey?.trim().replace(/[^\x00-\x7F]/g, '');
    
    console.log('Azure TTS request started:', {
      region,
      textLength: text.length,
      keyLength: cleanKey?.length,
      keyPreview: cleanKey?.substring(0, 10) + '...'
    });

    const response = await fetch(`https://${region}.tts.speech.microsoft.com/cognitiveservices/v1`, {
      method: 'POST',
      headers: {
        'Ocp-Apim-Subscription-Key': cleanKey,
        'Content-Type': 'application/ssml+xml',
        'X-Microsoft-OutputFormat': 'audio-16khz-128kbitrate-mono-mp3'
      },
      body: ssml,
    });

    if (!response.ok) {
      const errorText = await response.text().catch(() => 'ì‘ë‹µì„ ì½ì„ ìˆ˜ ì—†ìŒ');
      
      if (response.status === 401) {
        throw new Error(`ì¸ì¦ ì‹¤íŒ¨: API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš” (${response.status})`);
      } else if (response.status === 403) {
        throw new Error(`ê¶Œí•œ ê±°ë¶€: êµ¬ë… ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš” (${response.status})`);
      } else {
        throw new Error(`TTS ìš”ì²­ ì‹¤íŒ¨: ${response.status} - ${errorText}`);
      }
    }

    const audioBlob = await response.blob();
    if (audioBlob.size === 0) {
      throw new Error('ë¹ˆ ì˜¤ë””ì˜¤ íŒŒì¼ì´ ë°˜í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.');
    }

    return URL.createObjectURL(audioBlob);
  } catch (error) {
    console.error('Azure TTS ì˜¤ë¥˜:', error);
    throw error;
  }
};

export default ShadowingPracticeStep; 